---
title: "Understand the Incident"
date: 2022-11-03T10:57:46+01:00
weight: 102
---

## ðŸ“¥ STEP: Understand the Incident

>**ðŸš€ <u>Action</u>**
>Click twice on the  **Last occurrence** Header. 
>
>**Result**: The "Commit in repository robot-shop by Niklaus Hirt on file robot-shop.yaml" should be at the bottom* 

![image](./../images/demo/image.063.png)  

**ðŸ“£ <u>Narration</u>**

When trying to understand what happened during the incident, I sort the Alerts by occurence. This allows you to understand the chain of events.

* I can see that the first event was a code change that had been commited to **GitHub**. When I hover over the description I get the full text.
So it seems that the Development Team has reduced the available memory for the mysql database.

Other events are confirming the hypothesis. 
* I can then see the CI/CD process kick in and deploys the code change to the system detected by the Security tool and 
* **Instana** has has detected the memory size change. 


* Then **Functional Selenium Tests** start failing and 
* **Turbonomic** tries to scale-up the mysql database.
* **Instana** tells me that the mysql Pod is not running anymore, the replicas are not matching the desired state.

<div style="page-break-after: always;"></div>

* Cloud Pak for Watson AIOps has learned the normal patterns for logs coming from different applications. In this case, the Story also contains **Log Anomaly** Alerts (Summary: Abnormal behavior in the logs for component: ratings) that has been detected in the ratings service that cannot acces the mysql database. This detail is shown in the **insights.anomaly** section for the Alert **information**



## ðŸ“¥ STEP: Metric Anomaly

![image](./../images/demo/image.064.png)

>**ðŸš€ <u>Action</u>**
Click on one Alert line that has uppercase **ANOMALY:** in the Type column. Open the **Metric Anomaly Details** accordion and finally click on **View expanded chart**.


**ðŸ“£ <u>Narration</u>**

* Cloud Pak for Watson AIOps is also capable of collecting metrics from multiple sources and detecting **Metric Anomalies**. It was trained on hundreds or thousands of metrics from the environment and constructs a dynamic baseline (shown in green). The graphic suddenly turns red which relates to detected anomaly when the database is consuming a higher amount of memory than usual.


![image](./../images/demo/image.065.png)

>**ðŸš€ <u>Action</u>**
(1) In **Related Alerts** feel free to select some additional metric Alerts. Make sure your selection has a value under the Metric column. 


**ðŸ“£ <u>Narration</u>**

You can display several alerts at the same time to better understand the temporal dependencies

<div style="page-break-after: always;"></div>

>**ðŸš€ <u>Action</u>**
(2) Select a portion of the graph with your mouse to zoom in.


**ðŸ“£ <u>Narration</u>**

Now let's zoom in to better see the anomalies

![image](./../images/demo/image.066.png)

>**ðŸš€ <u>Action</u>**
Hover over a datapoint to show the actual metric value(s).


**ðŸ“£ <u>Narration</u>**

I can clearly see that the incident caused the **Latencies** to skyrocket and the **Transactions per Seconds** are almost zero. This is yet another confirmation of the root cause of the problem.

>**ðŸš€ <u>Action</u>**
Close the Metric anomaly details view. 